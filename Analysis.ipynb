{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TLUIc2VQKYN1",
        "outputId": "b2150ff2-0f2a-428b-d5d1-913a782322a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (1139, 11)\n",
            "\n",
            "Columns in the dataset:\n",
            "['Temperature', 'Approach', 'Project ID', 'Project Name', 'Human Rating', 'Prompt_Type', 'Generated Prompt', 'Generated Rating', 'Extracted Rating', 'Whitepaper Link', 'Re-Extracted Rating']\n",
            "\n",
            "Missing values in each column:\n",
            "Temperature              0\n",
            "Approach                 0\n",
            "Project ID               0\n",
            "Project Name             0\n",
            "Human Rating            80\n",
            "Prompt_Type              0\n",
            "Generated Prompt         0\n",
            "Generated Rating         0\n",
            "Extracted Rating        53\n",
            "Whitepaper Link          0\n",
            "Re-Extracted Rating    488\n",
            "dtype: int64\n",
            "\n",
            "--- Descriptive Statistics ---\n",
            "       Human Rating  Extracted Rating\n",
            "count   1059.000000       1086.000000\n",
            "mean       3.169877          3.510681\n",
            "std        0.695998          1.127722\n",
            "min        2.100000          1.000000\n",
            "25%        2.400000          2.500000\n",
            "50%        3.000000          4.000000\n",
            "75%        3.900000          4.500000\n",
            "max        4.400000          5.000000\n",
            "\n",
            "Correlation between Human and LLM ratings: 0.0132\n",
            "Number of samples used for correlation: 1008\n",
            "Mean Squared Error: 1.8558\n",
            "Root Mean Squared Error: 1.3623\n",
            "Mean Absolute Error: 1.1417\n",
            "\n",
            "--- Analysis by Approach ---\n",
            "         Human Rating                 Extracted Rating                \n",
            "                 mean       std count             mean       std count\n",
            "Approach                                                              \n",
            "1            3.162963  0.705143   216         4.174107  0.592213   224\n",
            "2            3.162963  0.705143   216         4.076303  0.686334   211\n",
            "3            3.174641  0.690727   627         3.099078  1.193791   651\n",
            "\n",
            "Rating Difference by Approach (Human - LLM):\n",
            "              mean       std  count\n",
            "Approach                           \n",
            "1        -1.014904  0.826446    208\n",
            "2        -0.916244  0.910394    197\n",
            "3         0.058043  1.403098    603\n",
            "\n",
            "--- Analysis by Prompt_Type ---\n",
            "            Human Rating                 Extracted Rating                \n",
            "                    mean       std count             mean       std count\n",
            "Prompt_Type                                                              \n",
            "CoT\\nsteps)     3.172453  0.698236   265         3.152747  1.170062   273\n",
            "CoT-CR          3.172453  0.698236   265         3.604000  1.177132   275\n",
            "Few-Shot        3.168182  0.696086   264         3.633582  1.046231   268\n",
            "Zero-Shot       3.166415  0.695361   265         3.655556  1.035551   270\n",
            "\n",
            "Rating Difference by Prompt Type (Human - LLM):\n",
            "                 mean       std  count\n",
            "Prompt_Type                           \n",
            "CoT\\nsteps)  0.005534  1.335760    253\n",
            "CoT-CR      -0.452157  1.350264    255\n",
            "Few-Shot    -0.466667  1.264645    249\n",
            "Zero-Shot   -0.503984  1.250849    251\n",
            "\n",
            "--- Combined Analysis: Approach x Prompt_Type ---\n",
            "                     Human Rating                 Extracted Rating            \\\n",
            "                             mean       std count             mean       std   \n",
            "Approach Prompt_Type                                                           \n",
            "1        CoT\\nsteps)     3.162963  0.710115    54         3.814286  0.667813   \n",
            "         CoT-CR          3.162963  0.710115    54         4.350000  0.583718   \n",
            "         Few-Shot        3.162963  0.710115    54         4.187500  0.510993   \n",
            "         Zero-Shot       3.162963  0.710115    54         4.344643  0.425513   \n",
            "2        CoT\\nsteps)     3.162963  0.710115    54         3.418519  0.878265   \n",
            "         CoT-CR          3.162963  0.710115    54         4.169643  0.489125   \n",
            "         Few-Shot        3.162963  0.710115    54         4.348000  0.326540   \n",
            "         Zero-Shot       3.162963  0.710115    54         4.403922  0.364396   \n",
            "3        CoT\\nsteps)     3.178981  0.694487   157         2.837423  1.272796   \n",
            "         CoT-CR          3.178981  0.694487   157         3.153374  1.280397   \n",
            "         Few-Shot        3.171795  0.690843   156         3.221605  1.121430   \n",
            "         Zero-Shot       3.168790  0.689654   157         3.184663  1.055765   \n",
            "\n",
            "                            \n",
            "                     count  \n",
            "Approach Prompt_Type        \n",
            "1        CoT\\nsteps)    56  \n",
            "         CoT-CR         56  \n",
            "         Few-Shot       56  \n",
            "         Zero-Shot      56  \n",
            "2        CoT\\nsteps)    54  \n",
            "         CoT-CR         56  \n",
            "         Few-Shot       50  \n",
            "         Zero-Shot      51  \n",
            "3        CoT\\nsteps)   163  \n",
            "         CoT-CR        163  \n",
            "         Few-Shot      162  \n",
            "         Zero-Shot     163  \n",
            "\n",
            "--- Statistical Tests ---\n",
            "\n",
            "ANOVA for Approach effect on rating difference:\n",
            "                  sum_sq      df          F        PR(>F)\n",
            "C(Approach)   255.505365     2.0  86.227766  2.746802e-35\n",
            "Residual     1488.980309  1005.0        NaN           NaN\n",
            "\n",
            "ANOVA for Prompt Type effect on rating difference:\n",
            "                     sum_sq      df         F    PR(>F)\n",
            "C(Prompt_Type)    43.967759     3.0  8.652977  0.000011\n",
            "Residual        1700.517916  1004.0       NaN       NaN\n",
            "\n",
            "Two-way ANOVA (Approach x Prompt Type) on rating difference:\n",
            "                                 sum_sq     df          F        PR(>F)\n",
            "C(Approach)                  255.924240    2.0  88.800617  3.250222e-36\n",
            "C(Prompt_Type)                44.386633    3.0  10.267519  1.164245e-06\n",
            "C(Approach):C(Prompt_Type)     9.352841    6.0   1.081750  3.713699e-01\n",
            "Residual                    1435.240835  996.0        NaN           NaN\n",
            "\n",
            "--- Error Analysis ---\n",
            "Top 10 largest rating discrepancies:\n",
            "      Project ID   Project Name  Approach  Prompt_Type  Human Rating  \\\n",
            "893         1979            LCX         3  CoT\\nsteps)           4.4   \n",
            "313         1979            LCX         3  CoT\\nsteps)           4.4   \n",
            "681           42   MEDICOHEALTH         2  CoT\\nsteps)           4.1   \n",
            "869         1533  Asure Network         3  CoT\\nsteps)           4.0   \n",
            "289         1533  Asure Network         3  CoT\\nsteps)           4.0   \n",
            "314         1979            LCX         3       CoT-CR           4.4   \n",
            "894         1979            LCX         3       CoT-CR           4.4   \n",
            "517         5469          VECAP         3  CoT\\nsteps)           3.9   \n",
            "1097        5469          VECAP         3  CoT\\nsteps)           3.9   \n",
            "857         1498       Etherisc         3  CoT\\nsteps)           4.3   \n",
            "\n",
            "      Extracted Rating  Absolute_Difference  \n",
            "893                1.0                  3.4  \n",
            "313                1.0                  3.4  \n",
            "681                1.0                  3.1  \n",
            "869                1.0                  3.0  \n",
            "289                1.0                  3.0  \n",
            "314                1.5                  2.9  \n",
            "894                1.5                  2.9  \n",
            "517                1.0                  2.9  \n",
            "1097               1.0                  2.9  \n",
            "857                1.5                  2.8  \n",
            "\n",
            "--- Qualitative Analysis of Generated Ratings ---\n",
            "\n",
            "Text Length Statistics:\n",
            "count    1139.000000\n",
            "mean     1045.765584\n",
            "std       416.467005\n",
            "min        99.000000\n",
            "25%       766.500000\n",
            "50%       950.000000\n",
            "75%      1278.000000\n",
            "max      2386.000000\n",
            "Name: Text_Length, dtype: float64\n",
            "\n",
            "Sentiment Score Statistics:\n",
            "count    1139.000000\n",
            "mean        0.884606\n",
            "std         0.252359\n",
            "min        -0.859100\n",
            "25%         0.913600\n",
            "50%         0.970100\n",
            "75%         0.986850\n",
            "max         0.997600\n",
            "Name: Sentiment_Score, dtype: float64\n",
            "\n",
            "Correlation between Sentiment and Human Rating: 0.0935\n",
            "Correlation between Sentiment and LLM Rating: 0.3636\n",
            "\n",
            "Common Themes by Approach:\n",
            "\n",
            "1:\n",
            "  - Positive: strong: 201\n",
            "  - Positive: clear: 111\n",
            "  - Positive: comprehensive: 65\n",
            "  - Negative: lack\\w*\\s\\w+: 47\n",
            "  - Positive: detailed: 41\n",
            "\n",
            "2:\n",
            "  - Positive: strong: 197\n",
            "  - Positive: detailed: 102\n",
            "  - Positive: clear: 98\n",
            "  - Positive: comprehensive: 88\n",
            "  - Positive: thorough: 21\n",
            "\n",
            "3:\n",
            "  - Positive: strong: 371\n",
            "  - Positive: clear: 342\n",
            "  - Negative: lack\\w*\\s\\w+: 325\n",
            "  - Positive: detailed: 158\n",
            "  - Positive: comprehensive: 78\n",
            "\n",
            "Rating extraction success rate: 87.88%\n",
            "Correlation between text-extracted ratings and provided extracted ratings: 0.9542\n",
            "\n",
            "Sentiment-Rating alignment rate: 44.75%\n",
            "\n",
            "Sentiment-Rating alignment by Approach:\n",
            "Approach\n",
            "1    75.000000\n",
            "2    68.720379\n",
            "3    26.574501\n",
            "Name: Sentiment_Rating_Aligned, dtype: float64\n",
            "\n",
            "Sentiment-Rating alignment by Prompt Type:\n",
            "Prompt_Type\n",
            "CoT\\nsteps)    27.106227\n",
            "CoT-CR         53.454545\n",
            "Few-Shot       46.268657\n",
            "Zero-Shot      52.222222\n",
            "Name: Sentiment_Rating_Aligned, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3900244317.py:421: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  consistency_df['Sentiment_Rating_Aligned'] = (\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis complete. All figures have been saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Quantitative & Qualitative analysis -- Digital Assets Paper - Code by Pejman Ebrahimi (pejman.ebrahimi@uni.li)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import re\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "\n",
        "# Set plotting style for publication quality\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
        "plt.rcParams['font.size'] = 11\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 10\n",
        "plt.rcParams['ytick.labelsize'] = 10\n",
        "plt.rcParams['legend.fontsize'] = 10\n",
        "plt.rcParams['figure.titlesize'] = 16\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "\n",
        "try:\n",
        "    nltk.data.find('vader_lexicon')\n",
        "except LookupError:\n",
        "    nltk.download('vader_lexicon')\n",
        "\n",
        "# Data\n",
        "df = pd.read_excel('Data.xlsx')\n",
        "\n",
        "# Clean up the Prompt Type column\n",
        "prompt_type_mapping = {\n",
        "    \"1. Zero-Shot (ZS) (step-by-step instructions)\": \"Zero-Shot\",\n",
        "    \"2. Few-shot (example of expected behaviors)\": \"Few-Shot\",\n",
        "    \"3. Chain-of-thoughts (sequence of intermediary reasoning\": \"CoT\",\n",
        "    \"4.Chain of thought including Problem context and rubric (CR)\": \"CoT-CR\"\n",
        "}\n",
        "\n",
        "df['Prompt Type'] = df['Prompt Type'].map(lambda x: prompt_type_mapping.get(x, x))\n",
        "\n",
        "# Force all variations of the long prompt type names to be the short names\n",
        "# This is a direct string replacement to ensure all instances are caught --> Short is better!\n",
        "df['Prompt Type'] = df['Prompt Type'].astype(str)\n",
        "df['Prompt Type'] = df['Prompt Type'].str.replace(\"3. Chain-of-thoughts (sequence of intermediary reasoning\", \"CoT\", regex=False)\n",
        "df['Prompt Type'] = df['Prompt Type'].str.replace(\"3. Chain-of-thoughts (sequence of intermediary reasoning steps)\", \"CoT\", regex=False)\n",
        "\n",
        "# Rename columns with spaces for statsmodels compatibility\n",
        "df = df.rename(columns={'Prompt Type': 'Prompt_Type'})\n",
        "\n",
        "\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nColumns in the dataset:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Just for check\n",
        "if df['Human Rating'].dtype == 'object':\n",
        "    df['Human Rating'] = pd.to_numeric(df['Human Rating'], errors='coerce')\n",
        "if df['Extracted Rating'].dtype == 'object':\n",
        "    df['Extracted Rating'] = pd.to_numeric(df['Extracted Rating'], errors='coerce')\n",
        "\n",
        "# Create a function to save high-quality figures\n",
        "def save_figure(fig, filename, dpi=300):\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(filename, dpi=dpi, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "# ===================== QUANTITATIVE ANALYSIS =====================\n",
        "\n",
        "# 1. Basic statistics for the ratings\n",
        "print(\"\\n--- Descriptive Statistics ---\")\n",
        "rating_stats = df[['Human Rating', 'Extracted Rating']].describe()\n",
        "print(rating_stats)\n",
        "\n",
        "# Distribution of ratings\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "sns.histplot(df['Human Rating'].dropna(), kde=True, ax=axes[0], color='blue')\n",
        "axes[0].set_title('Distribution of Human Ratings')\n",
        "axes[0].set_xlabel('Rating')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "sns.histplot(df['Extracted Rating'].dropna(), kde=True, ax=axes[1], color='green')\n",
        "axes[1].set_title('Distribution of LLM Ratings')\n",
        "axes[1].set_xlabel('Rating')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "save_figure(fig, 'rating_distributions.png')\n",
        "\n",
        "# 2. Correlation between Human and LLM ratings (Human & Machine)\n",
        "# Drop rows with missing values for correlation calculation --> another way is fillna (but for now just drop)\n",
        "corr_df = df.dropna(subset=['Human Rating', 'Extracted Rating'])\n",
        "correlation = corr_df['Human Rating'].corr(corr_df['Extracted Rating'])\n",
        "print(f\"\\nCorrelation between Human and LLM ratings: {correlation:.4f}\")\n",
        "print(f\"Number of samples used for correlation: {len(corr_df)}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.regplot(x='Human Rating', y='Extracted Rating', data=corr_df, scatter_kws={'alpha':0.5}, ax=ax)\n",
        "ax.set_title(f'Correlation between Human and LLM Ratings (r = {correlation:.4f})')\n",
        "ax.set_xlabel('Human Rating')\n",
        "ax.set_ylabel('LLM Rating')\n",
        "save_figure(fig, 'rating_correlation.png')\n",
        "\n",
        "# 3. Agreement metrics\n",
        "# Use only complete cases for error metrics\n",
        "mse = mean_squared_error(corr_df['Human Rating'], corr_df['Extracted Rating'])\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(corr_df['Human Rating'], corr_df['Extracted Rating'])\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "\n",
        "# 4. Analysis by Approach\n",
        "print(\"\\n--- Analysis by Approach ---\")\n",
        "approach_stats = df.groupby('Approach')[['Human Rating', 'Extracted Rating']].agg(['mean', 'std', 'count'])\n",
        "print(approach_stats)\n",
        "\n",
        "# Calculate rating difference by approach\n",
        "df['Rating_Difference'] = df['Human Rating'] - df['Extracted Rating']\n",
        "approach_diff = df.groupby('Approach')['Rating_Difference'].agg(['mean', 'std', 'count']).dropna()\n",
        "print(\"\\nRating Difference by Approach (Human - LLM):\")\n",
        "print(approach_diff)\n",
        "\n",
        "# Visualize ratings by approach\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "approach_means = df.groupby('Approach')[['Human Rating', 'Extracted Rating']].mean().reset_index()\n",
        "approach_means_melted = pd.melt(approach_means, id_vars='Approach',\n",
        "                               value_vars=['Human Rating', 'Extracted Rating'],\n",
        "                               var_name='Rating Type', value_name='Average Rating')\n",
        "\n",
        "sns.barplot(x='Approach', y='Average Rating', hue='Rating Type', data=approach_means_melted, ax=ax)\n",
        "ax.set_title('Average Ratings by Approach')\n",
        "ax.set_xlabel('Approach')\n",
        "ax.set_ylabel('Average Rating')\n",
        "ax.legend(title='Rating Type')\n",
        "save_figure(fig, 'ratings_by_approach.png')\n",
        "\n",
        "# 5. Analysis by Prompt Type\n",
        "print(\"\\n--- Analysis by Prompt_Type ---\")\n",
        "prompt_stats = df.groupby('Prompt_Type')[['Human Rating', 'Extracted Rating']].agg(['mean', 'std', 'count'])\n",
        "print(prompt_stats)\n",
        "\n",
        "prompt_diff = df.groupby('Prompt_Type')['Rating_Difference'].agg(['mean', 'std', 'count']).dropna()\n",
        "print(\"\\nRating Difference by Prompt Type (Human - LLM):\")\n",
        "print(prompt_diff)\n",
        "\n",
        "# Visualize ratings by prompt type\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "prompt_means = df.groupby('Prompt_Type')[['Human Rating', 'Extracted Rating']].mean().reset_index()\n",
        "prompt_means_melted = pd.melt(prompt_means, id_vars='Prompt_Type',\n",
        "                             value_vars=['Human Rating', 'Extracted Rating'],\n",
        "                             var_name='Rating Type', value_name='Average Rating')\n",
        "\n",
        "sns.barplot(x='Prompt_Type', y='Average Rating', hue='Rating Type', data=prompt_means_melted, ax=ax)\n",
        "ax.set_title('Average Ratings by Prompt Type')\n",
        "ax.set_xlabel('Prompt Type')\n",
        "ax.set_ylabel('Average Rating')\n",
        "ax.legend(title='Rating Type')\n",
        "save_figure(fig, 'ratings_by_prompt_type.png')\n",
        "\n",
        "# 6. Combined analysis: Approach x Prompt_Type\n",
        "print(\"\\n--- Combined Analysis: Approach x Prompt_Type ---\")\n",
        "combined_stats = df.groupby(['Approach', 'Prompt_Type'])[['Human Rating', 'Extracted Rating']].agg(['mean', 'std', 'count'])\n",
        "print(combined_stats)\n",
        "\n",
        "# Visualize the combined analysis\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "combined_means = df.groupby(['Approach', 'Prompt_Type'])[['Human Rating', 'Extracted Rating']].mean().reset_index()\n",
        "combined_means_melted = pd.melt(combined_means, id_vars=['Approach', 'Prompt_Type'],\n",
        "                               value_vars=['Human Rating', 'Extracted Rating'],\n",
        "                               var_name='Rating Type', value_name='Average Rating')\n",
        "\n",
        "g = sns.catplot(x='Prompt_Type', y='Average Rating', hue='Rating Type', col='Approach',\n",
        "           data=combined_means_melted, kind='bar', height=5, aspect=1.2)\n",
        "g.fig.suptitle('Average Ratings by Approach and Prompt Type', y=1.05, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig('ratings_by_approach_and_prompt.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# 7. Statistical tests\n",
        "print(\"\\n--- Statistical Tests ---\")\n",
        "\n",
        "# Use only complete cases for statistical tests\n",
        "complete_df = df.dropna(subset=['Rating_Difference'])\n",
        "\n",
        "# ANOVA for Approach effect on rating difference\n",
        "model = ols('Rating_Difference ~ C(Approach)', data=complete_df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "print(\"\\nANOVA for Approach effect on rating difference:\")\n",
        "print(anova_table)\n",
        "\n",
        "# ANOVA for Prompt_Type effect on rating difference\n",
        "model = ols('Rating_Difference ~ C(Prompt_Type)', data=complete_df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "print(\"\\nANOVA for Prompt Type effect on rating difference:\")\n",
        "print(anova_table)\n",
        "\n",
        "# Two-way ANOVA\n",
        "model = ols('Rating_Difference ~ C(Approach) + C(Prompt_Type) + C(Approach):C(Prompt_Type)', data=complete_df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "print(\"\\nTwo-way ANOVA (Approach x Prompt Type) on rating difference:\")\n",
        "print(anova_table)\n",
        "\n",
        "# 8. Error analysis - where are the largest discrepancies?\n",
        "df['Absolute_Difference'] = abs(df['Rating_Difference'])\n",
        "print(\"\\n--- Error Analysis ---\")\n",
        "print(\"Top 10 largest rating discrepancies:\")\n",
        "top_discrepancies = df.sort_values('Absolute_Difference', ascending=False).dropna(subset=['Absolute_Difference']).head(10)\n",
        "print(top_discrepancies[['Project ID', 'Project Name', 'Approach', 'Prompt_Type', 'Human Rating', 'Extracted Rating', 'Absolute_Difference']])\n",
        "\n",
        "# Visualize error distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.boxplot(x='Approach', y='Absolute_Difference', data=df.dropna(subset=['Absolute_Difference']), ax=ax)\n",
        "ax.set_title('Distribution of Rating Discrepancies by Approach')\n",
        "ax.set_xlabel('Approach')\n",
        "ax.set_ylabel('Absolute Difference |Human - LLM|')\n",
        "save_figure(fig, 'error_by_approach.png')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.boxplot(x='Prompt_Type', y='Absolute_Difference', data=df.dropna(subset=['Absolute_Difference']), ax=ax)\n",
        "ax.set_title('Distribution of Rating Discrepancies by Prompt Type')\n",
        "ax.set_xlabel('Prompt Type')\n",
        "ax.set_ylabel('Absolute Difference |Human - LLM|')\n",
        "save_figure(fig, 'error_by_prompt_type.png')\n",
        "\n",
        "# 9. Heatmap of average absolute differences\n",
        "pivot_approach_prompt = df.pivot_table(\n",
        "    values='Absolute_Difference',\n",
        "    index='Approach',\n",
        "    columns='Prompt_Type',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.heatmap(pivot_approach_prompt, annot=True, cmap='YlOrRd', fmt='.2f', ax=ax)\n",
        "ax.set_title('Average Absolute Rating Difference by Approach and Prompt Type')\n",
        "save_figure(fig, 'error_heatmap.png')\n",
        "\n",
        "# ===================== QUALITATIVE ANALYSIS =====================\n",
        "\n",
        "# Check if Generated Rating column exists and has data\n",
        "if 'Generated Rating' in df.columns and not df['Generated Rating'].isnull().all():\n",
        "    print(\"\\n--- Qualitative Analysis of Generated Ratings ---\")\n",
        "\n",
        "    # 1. Text length analysis\n",
        "    df['Text_Length'] = df['Generated Rating'].fillna('').apply(len)\n",
        "\n",
        "    print(\"\\nText Length Statistics:\")\n",
        "    print(df['Text_Length'].describe())\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.boxplot(x='Approach', y='Text_Length', data=df, ax=ax)\n",
        "    ax.set_title('Distribution of Generated Text Length by Approach')\n",
        "    ax.set_xlabel('Approach')\n",
        "    ax.set_ylabel('Text Length (characters)')\n",
        "    save_figure(fig, 'text_length_by_approach.png')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.boxplot(x='Prompt_Type', y='Text_Length', data=df, ax=ax)\n",
        "    ax.set_title('Distribution of Generated Text Length by Prompt Type')\n",
        "    ax.set_xlabel('Prompt Type')\n",
        "    ax.set_ylabel('Text Length (characters)')\n",
        "    save_figure(fig, 'text_length_by_prompt_type.png')\n",
        "\n",
        "    # 2. Sentiment analysis\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Apply sentiment analysis to each text\n",
        "    df['Sentiment_Score'] = df['Generated Rating'].fillna('').apply(\n",
        "        lambda x: sia.polarity_scores(x)['compound'] if isinstance(x, str) else np.nan\n",
        "    )\n",
        "\n",
        "    print(\"\\nSentiment Score Statistics:\")\n",
        "    print(df['Sentiment_Score'].describe())\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.boxplot(x='Approach', y='Sentiment_Score', data=df.dropna(subset=['Sentiment_Score']), ax=ax)\n",
        "    ax.set_title('Distribution of Sentiment Scores by Approach')\n",
        "    ax.set_xlabel('Approach')\n",
        "    ax.set_ylabel('Sentiment Score')\n",
        "    save_figure(fig, 'sentiment_by_approach.png')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.boxplot(x='Prompt_Type', y='Sentiment_Score', data=df.dropna(subset=['Sentiment_Score']), ax=ax)\n",
        "    ax.set_title('Distribution of Sentiment Scores by Prompt Type')\n",
        "    ax.set_xlabel('Prompt Type')\n",
        "    ax.set_ylabel('Sentiment Score')\n",
        "    save_figure(fig, 'sentiment_by_prompt_type.png')\n",
        "\n",
        "    # 3. Correlation between sentiment and ratings\n",
        "    sentiment_corr_df = df.dropna(subset=['Sentiment_Score', 'Human Rating'])\n",
        "    sentiment_human_corr = sentiment_corr_df['Sentiment_Score'].corr(sentiment_corr_df['Human Rating'])\n",
        "\n",
        "    sentiment_llm_corr_df = df.dropna(subset=['Sentiment_Score', 'Extracted Rating'])\n",
        "    sentiment_llm_corr = sentiment_llm_corr_df['Sentiment_Score'].corr(sentiment_llm_corr_df['Extracted Rating'])\n",
        "\n",
        "    print(f\"\\nCorrelation between Sentiment and Human Rating: {sentiment_human_corr:.4f}\")\n",
        "    print(f\"Correlation between Sentiment and LLM Rating: {sentiment_llm_corr:.4f}\")\n",
        "\n",
        "    # 4. Word frequency analysis\n",
        "    # Combine all text for each approach\n",
        "    approach_texts = {}\n",
        "    for approach in df['Approach'].unique():\n",
        "        approach_texts[approach] = ' '.join([\n",
        "            str(text) for text in df[df['Approach'] == approach]['Generated Rating'].dropna()\n",
        "            if isinstance(text, str)\n",
        "        ])\n",
        "\n",
        "    # Generate word clouds for each approach\n",
        "    for approach, text in approach_texts.items():\n",
        "        if text.strip():  # Check if text is not empty\n",
        "            wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(text)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 5))\n",
        "            ax.imshow(wordcloud, interpolation='bilinear')\n",
        "            ax.set_title(f'Word Cloud for Approach: {approach}')\n",
        "            ax.axis('off')\n",
        "            save_figure(fig, f'wordcloud_approach_{approach}.png')\n",
        "\n",
        "    # 5. Extract common phrases or themes\n",
        "    # Function to extract key phrases (simplified)\n",
        "    def extract_key_phrases(text):\n",
        "        # This is a simplified approach - in a real analysis, you might use NLP techniques\n",
        "        phrases = []\n",
        "        if isinstance(text, str):\n",
        "            # Look for phrases like \"the paper is well-written\" or \"lacks clarity\"\n",
        "            positive_patterns = [\n",
        "                r'well[\\s-]written', r'clear', r'comprehensive', r'excellent',\n",
        "                r'good', r'strong', r'thorough', r'detailed'\n",
        "            ]\n",
        "            negative_patterns = [\n",
        "                r'lack\\w*\\s\\w+', r'poor', r'weak', r'insufficient',\n",
        "                r'inadequate', r'missing', r'unclear', r'confusing'\n",
        "            ]\n",
        "\n",
        "            for pattern in positive_patterns:\n",
        "                if re.search(pattern, text, re.IGNORECASE):\n",
        "                    phrases.append(f\"Positive: {pattern}\")\n",
        "\n",
        "            for pattern in negative_patterns:\n",
        "                if re.search(pattern, text, re.IGNORECASE):\n",
        "                    phrases.append(f\"Negative: {pattern}\")\n",
        "\n",
        "        return phrases\n",
        "\n",
        "    # Apply to each text\n",
        "    df['Key_Phrases'] = df['Generated Rating'].apply(extract_key_phrases)\n",
        "\n",
        "    # Count phrase occurrences by approach\n",
        "    approach_phrases = {}\n",
        "    for approach in df['Approach'].unique():\n",
        "        phrases = []\n",
        "        for phrase_list in df[df['Approach'] == approach]['Key_Phrases']:\n",
        "            phrases.extend(phrase_list)\n",
        "\n",
        "        phrase_counts = Counter(phrases)\n",
        "        approach_phrases[approach] = phrase_counts\n",
        "\n",
        "    print(\"\\nCommon Themes by Approach:\")\n",
        "    for approach, phrases in approach_phrases.items():\n",
        "        print(f\"\\n{approach}:\")\n",
        "        for phrase, count in phrases.most_common(5):\n",
        "            print(f\"  - {phrase}: {count}\")\n",
        "\n",
        "    # 6. Rating extraction patterns\n",
        "    # Check if ratings are consistently formatted in the text\n",
        "    def extract_rating_from_text(text):\n",
        "        if not isinstance(text, str):\n",
        "            return None\n",
        "\n",
        "        # Look for patterns like \"rating: X\" or \"score: X\"\n",
        "        rating_patterns = [\n",
        "            r'rating:\\s*(\\d+(?:\\.\\d+)?)',\n",
        "            r'score:\\s*(\\d+(?:\\.\\d+)?)',\n",
        "            r'grade:\\s*(\\d+(?:\\.\\d+)?)',\n",
        "            r'(\\d+(?:\\.\\d+)?)\\s*out of\\s*\\d+'\n",
        "        ]\n",
        "\n",
        "        for pattern in rating_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                try:\n",
        "                    return float(match.group(1))\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    df['Text_Extracted_Rating'] = df['Generated Rating'].apply(extract_rating_from_text)\n",
        "\n",
        "    # Check how often we can extract ratings from text\n",
        "    extraction_success = df['Text_Extracted_Rating'].notnull().mean() * 100\n",
        "    print(f\"\\nRating extraction success rate: {extraction_success:.2f}%\")\n",
        "\n",
        "    # Compare extracted ratings with the provided extracted ratings\n",
        "    if extraction_success > 10:  # Only if we have a reasonable number of extracted ratings\n",
        "        text_vs_extracted_df = df.dropna(subset=['Text_Extracted_Rating', 'Extracted Rating'])\n",
        "        if len(text_vs_extracted_df) > 0:\n",
        "            text_vs_extracted_corr = text_vs_extracted_df['Text_Extracted_Rating'].corr(\n",
        "                text_vs_extracted_df['Extracted Rating']\n",
        "            )\n",
        "            print(f\"Correlation between text-extracted ratings and provided extracted ratings: {text_vs_extracted_corr:.4f}\")\n",
        "\n",
        "    # 7. Consistency analysis\n",
        "    # Check if the sentiment of the text aligns with the extracted rating\n",
        "    # Create a new dataframe with only rows that have both sentiment and extracted rating\n",
        "    consistency_df = df.dropna(subset=['Sentiment_Score', 'Extracted Rating'])\n",
        "\n",
        "    if len(consistency_df) > 0:\n",
        "        median_rating = consistency_df['Extracted Rating'].median()\n",
        "\n",
        "        consistency_df['Sentiment_Rating_Aligned'] = (\n",
        "            (consistency_df['Sentiment_Score'] > 0) & (consistency_df['Extracted Rating'] > median_rating) |\n",
        "            (consistency_df['Sentiment_Score'] < 0) & (consistency_df['Extracted Rating'] < median_rating) |\n",
        "            (abs(consistency_df['Sentiment_Score']) < 0.1) & (abs(consistency_df['Extracted Rating'] - median_rating) < 0.5)\n",
        "        )\n",
        "\n",
        "        alignment_rate = consistency_df['Sentiment_Rating_Aligned'].mean() * 100\n",
        "        print(f\"\\nSentiment-Rating alignment rate: {alignment_rate:.2f}%\")\n",
        "\n",
        "        # Alignment by approach\n",
        "        approach_alignment = consistency_df.groupby('Approach')['Sentiment_Rating_Aligned'].mean() * 100\n",
        "        print(\"\\nSentiment-Rating alignment by Approach:\")\n",
        "        print(approach_alignment)\n",
        "\n",
        "        # Alignment by prompt type\n",
        "        prompt_alignment = consistency_df.groupby('Prompt_Type')['Sentiment_Rating_Aligned'].mean() * 100\n",
        "        print(\"\\nSentiment-Rating alignment by Prompt Type:\")\n",
        "        print(prompt_alignment)\n",
        "\n",
        "        # 8. Visualize the alignment\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        sns.barplot(x='Approach', y='Sentiment_Rating_Aligned', data=consistency_df, estimator=lambda x: sum(x) / len(x) * 100, ax=ax)\n",
        "        ax.set_title('Sentiment-Rating Alignment by Approach')\n",
        "        ax.set_xlabel('Approach')\n",
        "        ax.set_ylabel('Alignment Rate (%)')\n",
        "        ax.set_ylim(0, 100)\n",
        "        save_figure(fig, 'sentiment_rating_alignment_by_approach.png')\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        sns.barplot(x='Prompt_Type', y='Sentiment_Rating_Aligned', data=consistency_df, estimator=lambda x: sum(x) / len(x) * 100, ax=ax)\n",
        "        ax.set_title('Sentiment-Rating Alignment by Prompt Type')\n",
        "        ax.set_xlabel('Prompt Type')\n",
        "        ax.set_ylabel('Alignment Rate (%)')\n",
        "        ax.set_ylim(0, 100)\n",
        "        save_figure(fig, 'sentiment_rating_alignment_by_prompt_type.png')\n",
        "else:\n",
        "    print(\"\\nThe 'Generated Rating' column is missing or empty. Skipping qualitative analysis.\")\n",
        "\n",
        "print(\"\\nAnalysis complete. All figures have been saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "siqffwync3Up"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}